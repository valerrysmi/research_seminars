### **Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation **

**Авторы:** Gavin C. Cawley, Nicola L. C. Talbot, Mark Girolami

---

#### **1. Введение и Мотивация**

*   **Проблема:** Многоклассовая логистическая регрессия (Multinomial Logistic Regression, MLR) — стандартный метод классификации. Однако в задачах с большим количеством признаков (например, тексты, микроматрицы) важна не только точность, но и **интерпретируемость** (отбор признаков) и **эффективность** (компактность и скорость работы модели).
*   **Существующее решение:** Для отбора признаков используется **L1-регуляризация (Лассо)**, которая "обнуляет" веса у маловажных признаков. Однако этот метод требует трудоёмкого подбора **гиперпараметра регуляризации `α`**, обычно с помощью перекрёстной проверки (Cross-Validation, CV), что вычислительно затратно.
*   **Цель работы:** Предложить метод **разреженной многоклассовой логистической регрессии с байесовской регуляризацией (SBMLR)**, который автоматически определяет силу регуляризации, интегрируя гиперпараметр `α` аналитически. Это позволяет избежать перекрёстной проверки и значительно ускорить обучение.

---

#### **2. Метод**

**2.1. Базовая модель (MLR + L1)**

*   **Вероятность класса:** Для объекта `xⁿ` вероятность отнести его к классу `i` вычисляется с помощью **softmax**:
    \[ p(t_i^n | x^n) = y_i^n = \frac{\exp\{a_i^n\}}{\sum_{j=1}^c \exp\{a_j^n\}} \quad \text{где} \quad a_i^n = \sum_{j=1}^d w_{ij}x_j^n \]
*   **Функция потерь (Логарифмическое правдоподобие):**
    \[ E_D = -\sum_{n=1}^\ell \sum_{i=1}^c t_i^n \log \{y_i^n\} \]
*   **Регуляризованный критерий:**
    \[ L = E_D + \alpha E_W \quad \text{где} \quad E_W = \sum_{i=1}^c \sum_{j=1}^d |w_{ij}| \]
    Здесь `α` — гиперпараметр, контролирующий компромисс между точностью и разреженностью.

**2.2. Байесовский подход: Интегрирование гиперпараметра**

*   **Ключевая идея:** Вместо подбора `α` рассматривать его как случайную величину и **интегрировать (усреднить)** по её априорному распределению.
*   **Априорное распределение:**
    *   На веса `w` задаётся **априор Лапласа**: `P(w|α) = (α/2)^W * exp(-α * E_W)`.
    *   На гиперпараметр `α` задаётся **неинформативное априорное распределение Джеффри**: `p(α) ∝ 1/α`.
*   **Маргинализация (интегрирование):** Вычисляется новый априор для весов, не зависящий от `α`:
    \[ p(w) = \int p(w|\alpha)p(\alpha)d\alpha = \frac{1}{2^W} \frac{\Gamma(W)}{E_W^W} \]
*   **Новый критерий для оптимизации:** Взяв отрицательный логарифм от `p(w)`, получаем новый функционал, в котором гиперпараметр `α` устранён:
    \[ M = E_D + W \log E_W \]
    где `W` — количество ненулевых весов.

**2.3. Практическая реализация алгоритма**

*   **Эквивалентность:** Минимизация нового критерия `M` эквивалентна минимизации исходного `L`, если на каждом шаге обновлять `α` по правилу:
    \[ \frac{1}{\tilde{\alpha}} = \frac{1}{W} \sum_{i=1}^{W} |w_i| \]
*   **Алгоритм обучения:** Используется модифицированный **координатный спуск** (циклическая оптимизация по одному весу за раз).
    *   **Градиенты:**
        \[ \frac{\partial E_D}{\partial w_{ij}} = \sum_{n=1}^\ell (y_i^n - t_i^n) x_j^n \]
        \[ \frac{\partial^2 E_D}{\partial w_{ij}^2} = \sum_{n=1}^\ell y_i^n (1 - y_i^n) (x_j^n)^2 \]
    *   **Обновление весов:** Методом Ньютона:
        \[ w_{ij} \leftarrow w_{ij} - \frac{\partial E_D}{\partial w_{ij}} \left[ \frac{\partial^2 E_D}{\partial w_{ij}^2} \right]^{-1} \]
    *   **Условия обнуления весов:** Вес обнуляется, если производная от регуляризатора "перевешивает" производную от правдоподобия.

---

#### **3. Результаты экспериментов**

Сравнение проводилось на 9 benchmark-датасетах между:
*   **SBMLR:** Предложенный метод (без CV).
*   **SMLR:** Стандартный метод с L1-регуляризацией и подбором `α` по 5-кратной CV.

**Ключевые выводы:**

1.  **Точность (Error Rate) и Перекрёстная энтропия (Cross-Entropy):** Методы показали **сопоставимые результаты**. Ни один из методов не доминировал consistently по всем наборам данных.
2.  **Разреженность (Sparsity):** SBMLR в большинстве случаев создавал **чуть более разреженные модели** (больше нулевых весов).
3.  **Скорость обучения:** SBMLR оказался **намного быстрее** — обычно в **~100 раз**, в худшем случае — в 5 раз.

---

#### **4. Преимущества вероятностной модели**

Поскольку SBMLR, как и любая логистическая регрессия, выдаёт вероятности, он позволяет:
*   **Классификация с минимальным риском:** Учитывать разную стоимость ошибок.
*   **Отказ от классификации:** Отсекать случаи с низкой уверенностью модели.
*   **Коррекция априорных вероятностей:** На этапе применения модели можно адаптировать выходные вероятности под изменённое распределение классов в данных (которое может отличаться от обучающей выборки). На примере датасета `covtype` такая коррекция позволила значительно снизить ошибку (с 40.51% до 28.57%).

---

#### **5. Сравнение с существующими подходами**

*   **LASSO, SMLR:** Требуют подбора `α` через CV.
*   **Relevance Vector Machine (RVM):** Использует другой априор (Student's t) и, наоборот, **интегрирует веса и оптимизирует гиперпараметры**. SBMLR же **интегрирует гиперпараметр и оптимизирует веса**.
*   **Метод Фигейредо (2003):** Также использует байесовский подход для бинарной классификации, но имеет вычислительную сложность `O(d³)`, что неприменимо для задач высокой размерности.

---

#### **6. Выводы**

*   **Основное достижение:** Предложен метод **SBMLR**, который позволяет строить разреженные многоклассовые модели логистической регрессии **без дорогостоящего подбора гиперпараметра регуляризации**.
*   **Результат:** Метод обеспечивает **сопоставимое качество** с подходом, использующим перекрёстную проверку, но при этом **значительно (до 100 раз) быстрее** и часто создаёт более разреженные модели.
*   **Применимость:** Метод особенно полезен в задачах с большим количеством признаков, где важны скорость, интерпретируемость и отбор признаков (например, биоинформатика, обработка текстов).

---
