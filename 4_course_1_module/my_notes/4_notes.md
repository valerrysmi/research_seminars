### **Efficient Feature Selection via Analysis of Relevance and Redundancy**

**Авторы:** Lei Yu, Huan Liu (Arizona State University)
**Основная идея:** Предложен новый эффективный метод отбора признаков для данных высокой размерности, который **явно разделяет анализ релевантности и избыточности**, что позволяет избежать дорогостоящего поиска по подмножествам.

---

### **1. Введение и Постановка Проблемы**

*   **Проблема:** Высокая размерность данных (тысячи признаков) ведёт к «проклятию размерности»: переобучению, увеличению времени обучения, снижению точности и интерпретируемости моделей.
*   **Две главные причины:**
    1.  **Нерелевантные признаки:** Не несут полезной информации для предсказания целевой переменной (шум).
    2.  **Избыточные признаки:** Несут информацию, уже содержащуюся в других признаках (дублирование, сильная корреляция).
*   **Цель отбора признаков:** Найти **минимальное оптимальное подмножество** признаков `G`, которое сохраняет исходное распределение классов `P(C|F)`.

---

### **2. Теоретическая основа: Релевантность и Избыточность**

#### **2.1. Релевантность Признаков**

Признаки делятся на три категории (John, Kohavi, Pfleger, 1994):
1.  **Сильно релевантные:** Незаменимы. Их удаление всегда ухудшает качество модели.
    `P(C | F_i, S_i) ≠ P(C | S_i)`
2.  **Слабо релевантные:** Не незаменимы при наличии всех других признаков, но могут быть полезны при отсутствии некоторых из них.
    `P(C | F_i, S_i) = P(C | S_i)`, но `∃ S_i' ⊂ S_i, such that P(C | F_i, S_i') ≠ P(C | S_i')`
3.  **Нерелевантные:** Бесполезны при любых условиях. Их удаление безопасно.
    `∀ S_i' ⊆ S_i, P(C | F_i, S_i') = P(C | S_i')`

#### **2.2. Избыточность Признаков и Марковское Одеяло**

*   **Марковское одеяло (Markov Blanket):** Для признака `F_i` это такое подмножество признаков `M_i` (не включающее `F_i`), которое полностью "закрывает" его. Если известно `M_i`, то знание `F_i` не даёт новой информации ни о классе `C`, ни о других признаках.
*   **Определение избыточного признака:** Признак является избыточным в текущем наборе `G`, если он **слабо релевантен** и имеет **Марковское одеяло** внутри `G`.

**Оптимальное подмножество** состоит из всех **сильно релевантных** и **слабо релевантных, но неизбыточных** признаков.

---

### **3. Новый Подход к Отбору Признаков**

#### **3.1. Ограничения Существующих Методов**

*   **Методы индивидуальной оценки (напр., ReliefF):** Быстрые (`O(N)`), но не устраняют избыточность, так как оценивают признаки по отдельности.
*   **Методы оценки подмножеств (напр., CFS, FOCUS):** Учитывают избыточность, но требуют поиска по подмножествам, что ведёт к высокой вычислительной сложности (`O(N^2)` или `O(2^N)`), что неприемлемо для данных с высокой размерностью.

#### **3.2. Предлагаемый Фреймворк**

Предлагается **декомпозиция** задачи на два последовательных шага:
1.  **Анализ Релевантности:** Быстро отфильтровать все **нерелевантные** признаки. На выходе — подмножество релевантных признаков.
2.  **Анализ Избыточности:** Среди релевантных признаков выявить и удалить **избыточные**.

Это позволяет избежать дорогостоящего поиска по подмножествам.

---

### **4. Алгоритм FCBF (Fast Correlation-Based Filter)**

Алгоритм реализует предложенный фреймворк, используя в качестве меры зависимости **Симметричную Неопределенность (Symmetric Uncertainty, SU)**.

*   **Симметричная Неопределенность (SU):** Нормализованная версия Information Gain. Устраняет смещение в сторону признаков с большим количеством значений. Значения в диапазоне `[0,1]`.
    `SU(X, Y) = 2 [ IG(X|Y) / (H(X) + H(Y)) ]`

#### **4.1. Два Типа Корреляции в FCBF**

*   **C-Correlation (`SU_i,c`):** Корреляция между признаком `F_i` и классом `C` (мера релевантности).
*   **F-Correlation (`SU_i,j`):** Корреляция между двумя признаками `F_i` и `F_j` (мера потенциальной избыточности).

#### **4.2. Шаги Алгоритма FCBF**

**Вход:** Обучающая выборка, порог релевантности `δ`.
**Выход:** Подмножество отобранных признаков `S_best`.

**Шаг 1: Отбор Релевантных Признаков**
1.  Для каждого признака `F_i` вычислить `SU_i,c`.
2.  Отобрать в список `S_list` только признаки, у которых `SU_i,c > δ`.
3.  Отсортировать `S_list` по убыванию `SU_i,c`.

**Шаг 2: Удаление Избыточных Признаков (Поиск "Преобладающих" Признаков)**
*   **Приближенное Марковское Одеяло:** `F_j` образует приближённое Марковское одеяло для `F_i`, если:
    1.  `SU_j,c >= SU_i,c` (`F_j` не менее релевантен, чем `F_i`)
    2.  `SU_i,j >= SU_i,c` (`F_j` сильно коррелирован с `F_i`)
*   **Преобладающий признак (Predominant Feature):** Признак, для которого в текущем наборе не найдётся приближённого Марковского одеяла.
*   **Процедура:**
    1.  Начать с первого признака в отсортированном списке (самый релевантный). Он автоматически становится преобладающим.
    2.  Проверить все последующие признаки в списке. Если текущий преобладающий признак образует для кого-то из них приближённое Марковское одеяло, удалить этот признак из списка.
    3.  Перейти к следующему признаку в оставшемся списке и повторить шаги.
    4.  Процесс завершается, когда все признаки в списке обработаны.

**Результат:** Список `S_best` содержит **преобладающие признаки** — релевантные и неизбыточные.

---

### **5. Эксперименты и Результаты**

#### **5.1. Сравниваемые Алгоритмы**
*   **ReliefF:** Индивидуальная оценка.
*   **CFS-SF:** Оценка подмножеств, жадный последовательный прямой отбор (Sequential Forward Selection).
*   **FOCUS-SF:** Оценка подмножеств, поиск минимального согласованного подмножества с SFS.
*   **FCBF(0):** Порог релевантности `δ = 0`.
*   **FCBF(log):** Порог релевантности `δ` устанавливается эвристически.

#### **5.2. Критерии Оценки**
*   **Эффективность (Efficiency):** Время работы.
*   **Результативность (Effectiveness):**
    *   На синтетических данных: Сравнение с заведомо известным оптимальным подмножеством.
    *   На реальных данных: Точность прогноза (NBC, C4.5) на отобранных признаках.

#### **5.3. Ключевые Выводы**

*   **Синтетические данные (Corral):**
    *   FCBF успешно удаляет нерелевантные и многие избыточные признаки.
    *   В некоторых сложных случаях (когда избыточность определяется группой признаков) может уступать, как и другие эвристические методы.

*   **Реальные данные (UCI, NIPS):**
    *   **Эффективность:** FCBF **значительно быстрее** CFS-SF и FOCUS-SF (иногда на порядки), и часто быстрее ReliefF. Установка порога (`FCBF(log)`) дополнительно ускоряет работу.
    *   **Результативность:**
        *   FCBF радикально сокращает размерность (оставляет 1-5% признаков).
        *   Точность моделей (NBC, C4.5) на признаках от FCBF **сравнима или выше**, чем на полном наборе признаков и на подмножествах от других алгоритмов.
        *   Для наивного байесовского классификатора (NBC) выигрыш в точности особенно заметен.

---

### **6. Заключение и Будущие Исследования**

*   **Итог:** Предложен новый, **эффективный и результативный** метод отбора признаков, который за счёт явного разделения анализа релевантности и избыточности хорошо подходит для данных высокой размерности.
*   **Преимущества FCBF:**
    *   Высокая скорость.
    *   Сильное сокращение размерности.
    *   Сохранение или улучшение точности прогноза.
    *   Независимость от алгоритма обучения (filter-метод).
*   **Направления будущих исследований:**
    *   Изучение влияния методов дискретизации на FCBF.
    *   Использование других мер корреляции для данных разных типов.
    *   Расширение метода на задачи регрессии.
    *   Применение к данным геномных микроматриц.

---
